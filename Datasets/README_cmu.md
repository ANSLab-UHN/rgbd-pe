# CMU folder
Instructions specific to the CMU Panoptic dataset processing.

Need to be in dite venv.

Annotations are provided by images IDs may be different than what is generated by following this step-by-step procedure. If the IDs are clashing, the steps need to be performed from the beginning.

My annotations are provided in the default_anns folder (both Dite-HRNet and MobileHumanPose format).


## 1. Download CMU Panoptic Dataset
https://github.com/CMU-Perceptual-Computing-Lab/panoptic-toolbox
* Download kinoptic data (for rgb-d data) and panoptic data (for ground truth)
* Scripts to extract data in the Git page

The folder should look like this

```
|---cmu
    |---git
        |---gloria_scripts
        |---matlab
        |---OGDATA
        |---OGSKELETON
        |---python
        |---scripts
        |---SKELETON
```

gloria_scripts contains code written by me. See instructions to see what code to place in that folder.

OGDATA contains kinoptic data (kinect_shared_depth, kinectVideos, calibration, kcalibration, syntables and ksyntables) - **RENAMED FROM GITHUB DOWNLOAD**

OGSKELETON contrains panoptic data (hdPose3d_stage1_coco19, calibration and tar files). hdVideos and vgaVideos were deleted (not needed) - **RENAMED FROM GITHUB DOWNLOAD**

SKELETON is a filtered version or OGSKELETON: (after applying REM_EMPTY_JSON.py)
* OGSKELETON data had to be filtered because of empty json files
* Place REM_EMPTY_JSON.py in the SKELETON folder and run it
* It will automatically create SKELETON

Now, 3 new folders need to be created: SKELETON/TRAIN, SKELETON/VAL and SKELETON/TEST. 

Create these folders and copy trials (from SKELETON) into them based on the desired train/val/test split.

* Data split used for my work:
    * Train: 
        * flute1
        * piano2, piano3, piano4 
        * cello3
        * office2 
        * 171026_pose1 and pose3
        * 171204_pose1, pose2, pose4 and pose5
    * Val: 
        * office1
        * 171026_pose2
        * 171204_pose3
    * Test: 
        * dance5
        * 171204_pose6


## 2. Align kinect frames with GT timestamp
Using **ALIGN_MOD.py** (run from SKELETON FOLDER)

This code creates dictionaries with each cmu file, timestamps and corresponding kframe number. It aligns the ground truth with corresponding depth files.


## 3. Rename kinect frames
Using **ID_AND_RENAME.py** (run from gloria_scripts folder)

This code loops through the dictionary created in Step 2 and creates IDs and jpeg files with the IDs in their name. It places the new image files into a specified folder.


## 4. Get bbox labels
Using **YOLOv7** and **MODIFIEDdetect.py**

Yolov7 repo: https://github.com/WongKinYiu/yolov7

MODIFIEDdetect was created to replace the detect code from the original repo. Place this file in the yolov7 folder. It created bounding boxes in a COCO-compatible format.

Run the following command to generate the yolov7 bbox labels:
`python MODIFIEDdetect.py --weights yolov7.pt --source $PATH_TO_IMAGES -- class 0 --save-txt --save-conf --device 0 --nosave`

Running the code above will create a file in the yolov7/runs/detect called exp[###]COCO_EQUIVALENT_BBOX.json. Keep track of which file corresponds to which images


## 5. Convert the CMU GT
Using **FINAL_GT_CONV.py** (run from python folder)

This code loops through the dictionary generated during Step 3 and converts the CMU ground truth to a COCO format as long as the frame name != 'skip'.

It also uses information from the bounding box files generated in Step 4.

After running these steps, we have: Ground truth annotation files in COCO-format, bounding box detection files, and a folder of all images in correct order and identified with a specific ID.


## 6. Create the RGB-D files
Files were created using **create_rgbd_data.py** (with a limit of 5500 mm for depth)


## 7. Create the MobileHumanPose equivalent GT
Using **create_CAMERA.py, create_DATA.py and create_JOINT.py**

This code creates MobileHumanPose equivalent ground truth, treating each trial as its own subject.
